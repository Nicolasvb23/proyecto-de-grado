{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of requirements (ONLY IN COLLAB)\n",
    "# !pip install mmh3==4.0.1\n",
    "# !pip install google-api-python-client==2.122.0\n",
    "# !pip install SPARQLWrapper==2.0.0\n",
    "# !pip install country-list==1.0.0\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install evaluate\n",
    "# !pip install bert_score\n",
    "#!pip install unidecode\n",
    "\n",
    "### (ONLY IN COLLAB)\n",
    "## Uncompress the zip with the code\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# os.chdir('/content')\n",
    "\n",
    "# # Ruta al archivo ZIP\n",
    "# zip_file_path = 'ProyectoDeGrado.zip'\n",
    "\n",
    "# # Ruta donde descomprimir los archivos (en este caso, el mismo /content)\n",
    "# extract_to_path = '/content'\n",
    "\n",
    "# # Descomprimir el archivo\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_to_path)\n",
    "\n",
    "# print(\"Archivos descomprimidos en:\", extract_to_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Autoload all modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Crear datasets folder\n",
    "import os\n",
    "if not os.path.exists(\"Datasets\"):\n",
    "    os.makedirs(\"Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Downloaders.full_data import FullDataDownloader\n",
    "from DatasetsUtils.Parsers.process_metadata import MetadataProcessor\n",
    "from DatasetsUtils.Parsers.process_tables import TableProcessor\n",
    "from DatasetsUtils.Parsers.select_tables_and_metadata import DatasetSelector\n",
    "\n",
    "interest_words = [\"Ministerio de Turismo\"]\n",
    "\n",
    "download_folder = f\"PipelineDatasets/DownloadedDatasets\"\n",
    "\n",
    "downloader = FullDataDownloader(interest_words)\n",
    "downloader.download_resources()\n",
    "metadata_processor = MetadataProcessor()\n",
    "metadata_processor.process_all()\n",
    "table_processor = TableProcessor()\n",
    "table_processor.process_directory()\n",
    "dataset_selector = DatasetSelector()\n",
    "dataset_selector.process_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Búsqueda de conjuntos de datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D3L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generación de indices LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from d3l.indexing.similarity_indexes import (\n",
    "    NameIndex,\n",
    "    FormatIndex,\n",
    "    ValueIndex,\n",
    "    EmbeddingIndex,\n",
    "    DistributionIndex,\n",
    ")\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "\n",
    "dataloader = CSVDataLoader(root_path=data_path, encoding=\"utf-8\")\n",
    "\n",
    "# Metrics\n",
    "dataloader.print_table_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Name Index\n",
    "Utiliza el análisis de q-gramas en los nombres de atributos para calcular la distancia de Jaccard entre sus conjuntos de q-gramas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, \"Name.lsh\")\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Format Index\n",
    "Identifica el formato de los datos a partir de expresiones regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, \"./format.lsh\")\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(\n",
    "        dataloader=dataloader, index_similarity_threshold=threshold\n",
    "    )\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Value Index\n",
    "Emplea tokens TF-IDF para representar valores, utilizando la distancia de Jaccard entre los tokens para evaluar la similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, \"./value.lsh\")\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(\n",
    "        dataloader=dataloader, index_similarity_threshold=threshold\n",
    "    )\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Distribution Index\n",
    "Evalúa la relación entre valores de atributos numéricos mediante la estadística de Kolmogorov-Smirnov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "distribution_lsh = os.path.join(result_path, \"./distribution.lsh\")\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(\n",
    "        dataloader=dataloader, index_similarity_threshold=threshold\n",
    "    )\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Embedding Index\n",
    "Determina la relación del contenido textual mediante la distancia coseno entre sus representaciones vectoriales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_lsh = os.path.join(result_path, \"./embedding.lsh\")\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(\n",
    "        dataloader=dataloader, index_similarity_threshold=threshold\n",
    "    )\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 2: Navegación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detección de la columna sujeto\n",
    "Identifica el tipo de columna y los scores de las columnas \"named entity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\n",
    "\n",
    "def subjectColDetection(DATA_PATH, RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = [\n",
    "            name for name in os.listdir(DATA_PATH) if \".ipynb_checkpoints\" not in name\n",
    "        ]\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table, SearchingWeb=False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in {tableName} : {e}\")\n",
    "                continue\n",
    "        with open(os.path.join(RESULT_PATH, \"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "\n",
    "\n",
    "SubjectCol_dict = subjectColDetection(data_path, \"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Utilizando los scores para las columnas \"named entity\", encuentra la columna sujeto para cada tabla (la que representa a la tabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = [\n",
    "    name for name in os.listdir(data_path) if \".ipynb_checkpoints\" not in name\n",
    "]\n",
    "subject_columns = []\n",
    "all_columns = []\n",
    "tables_without_ne = []\n",
    "\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    if NE_column_score.values():\n",
    "        max_score = max(NE_column_score.values())\n",
    "    else:\n",
    "        tables_without_ne.append(table)\n",
    "        continue\n",
    "    all_columns.extend(\n",
    "        [f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()]\n",
    "    )\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n",
    "print(\"Amount of tables that don't have NE columns: \", len(tables_without_ne))\n",
    "print(\"Tables without NE columns: \", tables_without_ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aurum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph, draw_interactive_network\n",
    "\n",
    "aurum_graph = buildGraph(\n",
    "    dataloader,\n",
    "    data_path,\n",
    "    [name_index, value_index],\n",
    "    target_path=\"Result\",\n",
    "    table_dict=SubjectCol_dict,\n",
    ")\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Obtiene el subgrafo dado por los nodos \"given_nodes\" y sus relacionados\n",
    "def subgraph(given_nodes, graph: nx.Graph()):\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = aurum_graph.subgraph(relevant_nodes).copy()\n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene solo los nodos que corresponden a subject_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_SC_graph = subgraph(subject_columns, aurum_graph)\n",
    "draw_interactive_network(result_SC_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subgrafo que contiene todos los nodos (uno por cada columna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_graph = subgraph(all_columns, aurum_graph)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga del LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar LLM\n",
    "\n",
    "from MetadataLLM.abstract import ModelManager\n",
    "import torch\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using devide:\", DEVICE)\n",
    "print(\"Number of cuda:\", torch.cuda.device_count())\n",
    "\n",
    "# Inicialización del modelo y tokenizador\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "access_token = \"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\"\n",
    "\n",
    "# Carga en el atributo de clase el modelo y el tokenizador\n",
    "ModelManager.initialize_model(model_name, access_token, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Parte 3: Anotación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning, updatePhase, fallBack\n",
    "from TableMiner.SearchOntology import SearchDBPedia\n",
    "\n",
    "\n",
    "# table_domains: nombre de las tablas\n",
    "table_domains = [\n",
    "    name for name in os.listdir(data_path) if \".ipynb_checkpoints\" not in name\n",
    "]\n",
    "for table in table_domains:\n",
    "    table_domains[table_domains.index(table)] = table[:-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Table Miner +\n",
    "Anota cada columna con una entidad de Wikidata, basándose en el contenido de cada celda de la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Genera anotaciones dada una tabla\n",
    "def table_annotation(tableName, subcol_dict):\n",
    "    tableD = dataloader.read_table(tableName)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    # Fase de aprendizaje\n",
    "    tableLearning = TableLearning(tableName, tableD, NE_column=NE_Score)\n",
    "    # Fase de actualización\n",
    "    print(\"starting learning phase\")\n",
    "    tableLearning.table_learning()\n",
    "    print(\"starting update phase\")\n",
    "    updatePhase(tableLearning)\n",
    "    fallBack(tableLearning, simple_mode=False)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guarda las anotaciones en un archivo\n",
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, \"rb\") as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning[table]\n",
    "    with open(target_file, \"wb\") as file:\n",
    "        pickle.dump(dict_annotation, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Guardar la salida estándar original\n",
    "original_stdout = sys.stdout\n",
    "\n",
    "# Abrir el archivo para escribir la salida\n",
    "with open(\"output.log\", \"w\") as f:\n",
    "    sys.stdout = f  # Redirigir la salida a output.log\n",
    "\n",
    "    learning = {}\n",
    "    for table in table_domains:\n",
    "        print(f\"\\n ---- \\n Starting learning for {table} \\n ---- \\n\")\n",
    "        learning[table] = table_annotation(table, SubjectCol_dict)\n",
    "\n",
    "    for table in table_domains:\n",
    "        store_learning(table, learning, \"Result\", \"annotationDict.pkl\")\n",
    "\n",
    "# Restaurar la salida estándar original\n",
    "sys.stdout = original_stdout\n",
    "\n",
    "print(\"Proceso finalizado. La salida ha sido guardada en output.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_salida_anotaciones(lista_tablas, dict_of_annotation, SubjectCol_dict):\n",
    "    estructura = {}\n",
    "\n",
    "    for nombre_tabla in lista_tablas:\n",
    "        estructura[nombre_tabla] = {}\n",
    "\n",
    "        # Obtener datos de anotación para la tabla específica\n",
    "        learningT = dict_of_annotation[nombre_tabla]\n",
    "        annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "        # Obtener tipos y puntuaciones de columnas\n",
    "        column_types = SubjectCol_dict[nombre_tabla + \".csv\"][0]\n",
    "        column_scores = SubjectCol_dict[nombre_tabla + \".csv\"][1]\n",
    "\n",
    "        tableDataframe = dataloader.read_table(nombre_tabla)\n",
    "        for col_index, col_type in column_types.items():\n",
    "            column = tableDataframe.iloc[:, col_index]\n",
    "            if col_index in annotation_class:\n",
    "                # Obtener conceptos y URIS\n",
    "                ColumnSemantics = list(\n",
    "                    annotation_class[col_index].get_winning_concepts()\n",
    "                )\n",
    "                mapping = annotation_class[col_index].get_mapping_id_label()\n",
    "                entities = [\n",
    "                    {\"uri\": item, \"concept\": concept}\n",
    "                    for concept in ColumnSemantics\n",
    "                    if concept in mapping\n",
    "                    for item in mapping[concept]\n",
    "                ]\n",
    "            else:\n",
    "                entities = []\n",
    "\n",
    "            # Agregar datos al diccionario de salida para la columna\n",
    "            estructura[nombre_tabla][column.name] = {\n",
    "                \"entities\": entities,\n",
    "                \"type\": col_type.name,\n",
    "            }\n",
    "\n",
    "    return estructura\n",
    "\n",
    "\n",
    "with open(\"Result/annotationDict.pkl\", \"rb\") as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "\n",
    "# genero las salidas\n",
    "annotations = generar_salida_anotaciones(table_domains, learning, SubjectCol_dict)\n",
    "\n",
    "# Imprimir salida en formato JSON\n",
    "import json\n",
    "with open(\"annotations_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(annotations, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardo queries y resultados en cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TableMiner.Cache.cache_handler import OntologyRequestHandler\n",
    "\n",
    "ontology_request_handler = OntologyRequestHandler(\"Result\", \"ontologyRequests.pkl\")\n",
    "\n",
    "# Cargar solicitudes\n",
    "request_cache = ontology_request_handler.load_ontology_requests()\n",
    "ontology_request_handler.pretty_print_json(request_cache.get(\"searches\", {}))\n",
    "\n",
    "# Mostrar estadísticas de llamadas a la red\n",
    "ontology_request_handler.display_network_calls()\n",
    "\n",
    "# Guardar solicitudes\n",
    "ontology_request_handler.store_ontology_requests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Construyo el submission_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Guardo las anotaciones en un csv, como las recibe el evaluador\n",
    "En formato: TABLE, COL_ID, ANNOTATION\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "def findAnnotation(dict_of_annotation, tableList):\n",
    "    data = []\n",
    "    for tableN in tableList:\n",
    "        learningT = learning[tableN]\n",
    "        annotation_class = learningT.get_annotation_class()\n",
    "\n",
    "        for columnIndex, learning_class in annotation_class.items():\n",
    "            tableDataframe = dataloader.read_table(tableN)\n",
    "            col_name = tableDataframe.columns[columnIndex]\n",
    "            column = tableDataframe.iloc[:, columnIndex]\n",
    "            ColumnSemantics = learning_class.get_winning_concepts()\n",
    "            label_id_mapping = learning_class.get_mapping_id_label()\n",
    "            # Guarda el identificador y anotaciones en el formato esperado\n",
    "            annotations = \" \".join(\n",
    "                \" \".join(map(str, label_id_mapping[concept])) \n",
    "                for concept in ColumnSemantics if concept in label_id_mapping\n",
    "            )\n",
    "            row = {\n",
    "                \"tab_id\": tableN,\n",
    "                \"col_id\": columnIndex,\n",
    "                \"annotations\": annotations\n",
    "            }\n",
    "            data.append(row)\n",
    "\n",
    "    # Crea un DataFrame y guárdalo en CSV\n",
    "    df_annotations = pd.DataFrame(data)\n",
    "    df_annotations.to_csv(\"submission_annotations.csv\", index=False, header=False)\n",
    "    print(\"Archivo de anotaciones generado como 'submission_annotations.csv'.\")\n",
    "\n",
    "# Ejecuta la función con el archivo de anotaciones\n",
    "with open(\"Result/annotationDict.pkl\", 'rb') as file:\n",
    "    dict_annotation = pickle.load(file)\n",
    "    \n",
    "findAnnotation(dict_annotation, table_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluación SemTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.CTA_WD_Evaluator import CTA_Evaluator\n",
    "\n",
    "# Inicializa el evaluador con el archivo de ground truth\n",
    "answer_file_path = \"evaluation/cta_gt.csv\"\n",
    "cta_evaluator = CTA_Evaluator(answer_file_path)\n",
    "\n",
    "# Define el payload del cliente\n",
    "client_payload = {\n",
    "    \"submission_file_path\": \"submission_annotations.csv\",\n",
    "    \"aicrowd_submission_id\": 1234,\n",
    "    \"aicrowd_participant_id\": 5678\n",
    "}\n",
    "\n",
    "# Ejecuta la evaluación\n",
    "result = cta_evaluator._evaluate(client_payload, _context={})\n",
    "\n",
    "# Muestra los resultados\n",
    "print(\"Resultados de la evaluación:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluación anotaciones Catálogo de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.CTA_Catalogo_Evaluator import CTA_Evaluator\n",
    "\n",
    "# Inicializa el evaluador con el archivo de ground truth\n",
    "answer_file_path = \"evaluation/cta_catalogo_gt.csv\"\n",
    "cta_evaluator = CTA_Evaluator(answer_file_path)\n",
    "\n",
    "# Define el payload del cliente\n",
    "client_payload = {\n",
    "    \"submission_file_path\": \"submission_annotations.csv\",\n",
    "    \"aicrowd_submission_id\": 1234,\n",
    "    \"aicrowd_participant_id\": 5678\n",
    "}\n",
    "\n",
    "# Ejecuta la evaluación\n",
    "result = cta_evaluator._evaluate(client_payload, _context={})\n",
    "\n",
    "# Muestra los resultados\n",
    "print(\"Resultados de la evaluación:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluación tipos Catálogo de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.TYPE_Catalogo_Evaluator import TypeEvaluator\n",
    "\n",
    "# Inicializa el evaluador con el archivo de ground truth\n",
    "answer_file_path = \"evaluation/type_catalogo_gt.csv\"\n",
    "evaluator = TypeEvaluator(answer_file_path)\n",
    "\n",
    "# Ejecuta la evaluación\n",
    "result = evaluator.evaluate(\"submission_type_annotations.csv\")\n",
    "\n",
    "# Muestra los resultados\n",
    "print(\"Resultados de la evaluación:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: LLM metadata generator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.Classificators.classificator import FileClassifier\n",
    "from DatasetsUtils.helper import write_file, read_file, detect_encoding\n",
    "import json\n",
    "\n",
    "# Cargar el clasificador, con la palabra de interes usada\n",
    "classifier = FileClassifier()\n",
    "\n",
    "files_with_metadata, files_with_notes, files_with_both, files_with_nothing = (\n",
    "    classifier.run()\n",
    ")\n",
    "print(\"Files with metadata: \", files_with_metadata)\n",
    "print(\"Count: \", len(files_with_metadata), \"\\n\")\n",
    "print(\"Files with notes: \", files_with_notes)\n",
    "print(\"Count: \", len(files_with_notes), \"\\n\")\n",
    "print(\"Files with both: \", files_with_both)\n",
    "print(\"Count: \", len(files_with_both), \"\\n\")\n",
    "print(\"Files with nothing: \", files_with_nothing)\n",
    "print(\"Count: \", len(files_with_nothing), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_additional_info(directory):\n",
    "    \"\"\"Loads the additional_info.json file from the directory.\"\"\"\n",
    "    filepath = os.path.join(directory, \"additional_info.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"additional_info.json not found in {directory}\")\n",
    "    return read_file(filepath, \"json\")\n",
    "\n",
    "\n",
    "datasets_directory = \"PipelineDatasets/SelectedDatasets\"\n",
    "enriched_datasets_directory = \"PipelineDatasets/EnrichedDatasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripcion sin metadata\n",
    "\n",
    "Para los que no tienen ni notes ni metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description import TableDescriptionGenerator\n",
    "\n",
    "table_description_generator = TableDescriptionGenerator(DEVICE)\n",
    "\n",
    "table_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"tabla\": \"\"\"\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        \"\"\",\n",
    "        \"descripcion_salida\": \"Datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\",\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"ventas_gas_natural\",\n",
    "        \"nombre_recurso\": \"Ventas Gas Natural - Volúmenes por zona geográfica\",\n",
    "        \"tabla\": \"\"\"\n",
    "          Mes,Año,Zona,TransporteFirme,TransporteInterrumpible,GasConsumido\n",
    "          \"1\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"267638\"\n",
    "          \"1\";\"2019\";\"SUR\";\"9913738\";\"113289\";\"2341025\"\n",
    "          \"2\";\"2019\";\"LITORAL\";\"1584100\";\"0\";\"177916\"\n",
    "          \"2\";\"2019\";\"SUR\";\"8954344\";\"101339\";\"2408347\"\n",
    "          \"3\";\"2019\";\"LITORAL\";\"1753825\";\"0\";\"311369\"\n",
    "          \"5\";\"2019\";\"LITORAL\";\"1605800\";\"0\";\"355121\"\n",
    "        \"\"\",\n",
    "        \"descripcion_salida\": \"Datos de ventas de gas natural por mes, año, zona geográfica, transporte firme, transporte interrumpible y gas consumido\",\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "    table_description = table_description_generator.generate_description(\n",
    "        table, table_id, additional_info, table_description_few_shots_prompt_data\n",
    "    )\n",
    "    generated_table_descriptions[package_id] = table_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "\n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "\n",
    "    write_file(\n",
    "        os.path.join(output_directory_package, \"additional_info.json\"),\n",
    "        additional_info,\n",
    "        \"json\",\n",
    "        \"utf-8\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata (Column description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_description import ColumnDescriptionGenerator\n",
    "\n",
    "column_description_generator = ColumnDescriptionGenerator(DEVICE)\n",
    "\n",
    "column_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"medicinas\",\n",
    "        \"nombre_recurso\": \"Recursos medicinales por codigo.\",\n",
    "        \"contexto\": \"Datos de productos medicinales, que incluyen información sobre el nombre del producto, el código ATC, la vía de administración y la dosis recomendada\",\n",
    "        \"tabla\": \"\"\"\n",
    "          producto, codigo, via, dosis\n",
    "          Paracetamol, N02BE01, Oral, 500mg\n",
    "          Ibuprofeno, M01AE01, Oral, 200mg\n",
    "          Amoxicilina, J01CA04, Oral, 500mg\n",
    "          Metformina, A10BA02, Oral, 850mg\n",
    "        \"\"\",\n",
    "        \"columna_de_interes\": \"via\",\n",
    "        \"descripcion_salida\": \"Esta columna contiene información sobre la vía de administración de los productos medicinales\",\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"Distribución porcentual del Gasto Publico Social (GPS)\",\n",
    "        \"nombre_recurso\": \"GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "        \"contexto\": \"Datos de la distribución porcentual del Gasto Público Social (GPS) en asistencia y seguridad social según los principales incisos\",\n",
    "        \"tabla\": \"\"\"\n",
    "            Incisos principales GPS,año,valor\n",
    "            BPS,2018,78.7\n",
    "            Ministerio de Desarrollo Social,2010,1.8\n",
    "            Ministerio de Desarrollo Social,2018,3.4\n",
    "            Otros,2015,13.8\n",
    "            Transferencias a la seguridad social,2010,1.1\n",
    "            BPS,2017,79.2\n",
    "            Otros,2014,13.5\n",
    "            Ministerio de Desarrollo Social,2012,2.1\n",
    "            Inau,2014,2.9\n",
    "            Ministerio de Desarrollo Social,2011,1.9\n",
    "        \"\"\",\n",
    "        \"columna_de_interes\": \"Incisos principales GPS\",\n",
    "        \"descripcion_salida\": \"Incisos principales del Gasto Público Social (GPS) en asistencia y seguridad social\",\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"Distribución porcentual del Gasto Publico Social (GPS)\",\n",
    "        \"nombre_recurso\": \"GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "        \"contexto\": \"Datos de la distribución porcentual del Gasto Público Social (GPS) en asistencia y seguridad social según los principales incisos\",\n",
    "        \"tabla\": \"\"\"\n",
    "            Incisos principales GPS,año,valor\n",
    "            BPS,2018,78.7\n",
    "            Ministerio de Desarrollo Social,2010,1.8\n",
    "            Ministerio de Desarrollo Social,2018,3.4\n",
    "            Otros,2015,13.8\n",
    "            Transferencias a la seguridad social,2010,1.1\n",
    "            BPS,2017,79.2\n",
    "            Otros,2014,13.5\n",
    "            Ministerio de Desarrollo Social,2012,2.1\n",
    "            Inau,2014,2.9\n",
    "            Ministerio de Desarrollo Social,2011,1.9\n",
    "        \"\"\",\n",
    "        \"columna_de_interes\": \"año\",\n",
    "        \"descripcion_salida\": \"Año en el cual se realizó la medida\",\n",
    "    },\n",
    "    {\n",
    "        \"nombre_tabla\": \"Distribución porcentual del Gasto Publico Social (GPS)\",\n",
    "        \"nombre_recurso\": \"GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "        \"contexto\": \"Datos de la distribución porcentual del Gasto Público Social (GPS) en asistencia y seguridad social según los principales incisos\",\n",
    "        \"tabla\": \"\"\"\n",
    "            Incisos principales GPS,año,valor\n",
    "            BPS,2018,78.7\n",
    "            Ministerio de Desarrollo Social,2010,1.8\n",
    "            Ministerio de Desarrollo Social,2018,3.4\n",
    "            Otros,2015,13.8\n",
    "            Transferencias a la seguridad social,2010,1.1\n",
    "            BPS,2017,79.2\n",
    "            Otros,2014,13.5\n",
    "            Ministerio de Desarrollo Social,2012,2.1\n",
    "            Inau,2014,2.9\n",
    "            Ministerio de Desarrollo Social,2011,1.9\n",
    "        \"\"\",\n",
    "        \"columna_de_interes\": \"valor\",\n",
    "        \"descripcion_salida\": \"Valor porcentual de la distribución del Gasto Público Social (GPS) en ese inciso\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "column_descriptions = {}\n",
    "\n",
    "for package_id in files_with_notes:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(\n",
    "            table,\n",
    "            table_id,\n",
    "            col,\n",
    "            additional_info,\n",
    "            column_description_few_shots_prompt_data,\n",
    "        )\n",
    "        column_descriptions[package_id][col] = column_description\n",
    "\n",
    "# Files with nothing con notes ya generadas\n",
    "for package_id in files_with_nothing:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    enriched_directory = os.path.join(enriched_datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(enriched_directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "    columnas = table.columns\n",
    "    column_descriptions[package_id] = {}\n",
    "    for col in columnas:\n",
    "        column_description = column_description_generator.generate_column_description(\n",
    "            table,\n",
    "            table_id,\n",
    "            col,\n",
    "            additional_info,\n",
    "            column_description_few_shots_prompt_data,\n",
    "        )\n",
    "        column_descriptions[package_id][col] = column_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear archivo de metadata con las descripciones de las columnas, tipos y entidades anotadas\n",
    "# El archivo de metadata es un JSON\n",
    "output_directory = os.path.join(enriched_datasets_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "concatenated_lists = files_with_notes + files_with_nothing\n",
    "\n",
    "for package_id in concatenated_lists:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    metadata_file_path = os.path.join(directory, \"metadata_generated.json\")\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "    columnas = table.columns\n",
    "    # Cargar el JSON de metadata file con los datos\n",
    "    metadata_file = {}\n",
    "    metadata_file[\"atributos\"] = []\n",
    "\n",
    "    for col in columnas:\n",
    "        column_description = column_descriptions[package_id][col]\n",
    "        if (\n",
    "            annotations.get(f\"table_{table_id}\", {}).get(col, {}).get(\"entities\", [{}])\n",
    "            == []\n",
    "        ):\n",
    "            recursoRelacionado = \"\"\n",
    "        else:\n",
    "            recursoRelacionado = (\n",
    "                annotations.get(f\"table_{table_id}\", {})\n",
    "                .get(col, {})\n",
    "                .get(\"entities\", [{}])[0]\n",
    "                .get(\"uri\", \"\")\n",
    "            )\n",
    "        tipoDeDato = (\n",
    "            annotations.get(f\"table_{table_id}\", {}).get(col, {}).get(\"type\", \"\")\n",
    "        )\n",
    "        atributo = {\n",
    "            \"descripcion\": column_description,\n",
    "            \"tipoDeDato\": tipoDeDato,\n",
    "            \"nombreDeAtributo\": col,\n",
    "            \"informacionAdicional\": \"\",\n",
    "            \"recursoRelacionado\": recursoRelacionado,\n",
    "        }\n",
    "        metadata_file[\"atributos\"].append(atributo)\n",
    "\n",
    "    write_file(\n",
    "        os.path.join(output_directory, package_id, \"metadata_generated.json\"),\n",
    "        metadata_file,\n",
    "        \"json\",\n",
    "        \"utf-8\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción usando metadata\n",
    "\n",
    "Para los que tienen metadata pero no notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar descripciones para los que no tienen nada. Primero se genera la descripcion de la tabla, para tomar contexto general,\n",
    "# y luego metadata más especifica de cada columna.\n",
    "from MetadataLLM.table_description_with_metadata import (\n",
    "    TableDescriptionWithMetadataGenerator,\n",
    ")\n",
    "\n",
    "table_description_generator = TableDescriptionWithMetadataGenerator(DEVICE)\n",
    "\n",
    "metadata_description_few_shots_prompt_data = [\n",
    "    {\n",
    "        \"nombre_tabla\": \"Distribución porcentual del Gasto Publico Social (GPS)\",\n",
    "        \"nombre_recurso\": \"GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "        \"tabla\": \"\"\"\n",
    "            Incisos principales GPS,año,valor\n",
    "            BPS,2018,78.7\n",
    "            Ministerio de Desarrollo Social,2010,1.8\n",
    "            Ministerio de Desarrollo Social,2018,3.4\n",
    "            Otros,2015,13.8\n",
    "            Transferencias a la seguridad social,2010,1.1\n",
    "            BPS,2017,79.2\n",
    "            Otros,2014,13.5\n",
    "            Ministerio de Desarrollo Social,2012,2.1\n",
    "            Inau,2014,2.9\n",
    "            Ministerio de Desarrollo Social,2011,1.9\n",
    "            Transferencias a la seguridad social,2005,0.2\n",
    "            Otros,2012,13.4\n",
    "            Inau,2015,2.9\n",
    "            BPS,2012,80.3\n",
    "            Inau,2013,2.9\n",
    "            Transferencias a la seguridad social,2016,1.1\n",
    "            Transferencias a la seguridad social,2017,1.0\n",
    "            Transferencias a la seguridad social,2015,1.2\n",
    "            BPS,2016,79.4\n",
    "            Transferencias a la seguridad social,2014,1.2\n",
    "            Ministerio de Desarrollo Social,2015,2.5\n",
    "        \"\"\",\n",
    "        \"metadata_files\": [\n",
    "            \"\"\"\n",
    "            {\n",
    "            \"atributos\": [\n",
    "                {\n",
    "                \"descripcion\": \"Incisos principales GPS\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"String\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"Incisos principales GPS\"\n",
    "                },\n",
    "                {\n",
    "                \"descripcion\": \"año\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Number\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"año\"\n",
    "                },\n",
    "                {\n",
    "                \"descripcion\": \"valor\",\n",
    "                \"informacionAdicional\": \"\",\n",
    "                \"tipoDeDato\": \"Number\",\n",
    "                \"recursoRelacionado\": \"\",\n",
    "                \"nombreDeAtributo\": \"valor\"\n",
    "                }\n",
    "            ],\n",
    "            \"titulo\": \"Metadatos de Distribución porcentual del GPS en Asistencia y Seguridad Social según principales incisos\",\n",
    "            \"descripcion\": \"Distribución porcentual del GPS en Asistencia y Seguridad Social según los principales incisos que lo ejecutan\",\n",
    "            \"calculo\": \"(GPS en cada inciso/Total de GPS en Asistencia y Seguridad Social)*100\",\n",
    "            \"unidad\": \"Porcentaje\",\n",
    "            \"fuente\": \"MIDES Dirección Nacional de Evaluación y Monitoreo\",\n",
    "            \"direccion\": \"Dirección Nal. de Evaluación y Monitoreo\",\n",
    "            }\n",
    "            \"\"\"\n",
    "        ],\n",
    "        \"descripcion_salida\": \"\"\"Distribución porcentual del Gasto Publico Social (GPS) en Asistencia y Seguridad Social según los principales incisos que lo ejecutan\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "generated_table_descriptions = {}\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    table_resources = additional_info.get(\"table_resources\", {})\n",
    "    metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "\n",
    "    if len(table_resources) == 0:\n",
    "        print(f\"No resources found for package {package_id}\")\n",
    "        continue\n",
    "\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(table_resources.keys())[0]\n",
    "    table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "    # Tomamos la primera key de metadata_resources\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    metadata = read_file(\n",
    "        os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"json\"\n",
    "    )\n",
    "\n",
    "    table_description = table_description_generator.generate_description_with_metadata(\n",
    "        table,\n",
    "        table_id,\n",
    "        metadata,\n",
    "        additional_info,\n",
    "        metadata_description_few_shots_prompt_data,\n",
    "    )\n",
    "    generated_table_descriptions[package_id] = table_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar las descripciones generadas\n",
    "output_directory = os.path.join(enriched_datasets_directory)\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for package_id in files_with_metadata:\n",
    "    directory = os.path.join(datasets_directory, package_id)\n",
    "    additional_info = load_additional_info(directory)\n",
    "    additional_info[\"notes\"] = generated_table_descriptions[package_id]\n",
    "\n",
    "    output_directory_package = os.path.join(output_directory, package_id)\n",
    "    os.makedirs(output_directory_package, exist_ok=True)\n",
    "\n",
    "    write_file(\n",
    "        os.path.join(output_directory_package, \"additional_info.json\"),\n",
    "        additional_info,\n",
    "        \"json\",\n",
    "        \"utf-8\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unificar resultados en FinalMetadata\n",
    "\n",
    "Mergear lo generado en EnrichedDatasets con lo que se mantuvo de SelectedDatasets\n",
    "y crear FinalDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "\n",
    "def copy_directory(src, dest):\n",
    "    \"\"\"Copy a directory and its contents to another directory.\n",
    "    If the destination directory already exists, it will be replaced.\n",
    "    \"\"\"\n",
    "    if os.path.exists(dest):\n",
    "        shutil.rmtree(dest)\n",
    "    shutil.copytree(src, dest)\n",
    "\n",
    "\n",
    "final_datasets_directory = \"PipelineDatasets/FinalDatasets\"\n",
    "\n",
    "os.makedirs(final_datasets_directory, exist_ok=True)\n",
    "\n",
    "# Copiar los archivos de SelectedDatasets a FinalDatasets\n",
    "selected_src = os.path.join(datasets_directory)\n",
    "final_dest = os.path.join(final_datasets_directory)\n",
    "copy_directory(selected_src, final_dest)\n",
    "\n",
    "# Buscar los directorios en EnrichedDatasets y sobreescribir los archivos en FinalDatasets\n",
    "enriched_src = os.path.join(enriched_datasets_directory)\n",
    "if os.path.exists(enriched_src):\n",
    "    for package_id in os.listdir(enriched_src):\n",
    "        print(f\"Processing package {package_id}\")\n",
    "        package_src = os.path.join(enriched_src, package_id)\n",
    "        package_dest = os.path.join(final_dest, package_id)\n",
    "\n",
    "        # Asegurar que el directorio de destino exista\n",
    "        os.makedirs(package_dest, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(os.path.join(package_src, \"additional_info.json\")):\n",
    "            print(\"Copying additional_info.json\")\n",
    "            shutil.copy(os.path.join(package_src, \"additional_info.json\"), package_dest)\n",
    "\n",
    "        if os.path.exists(os.path.join(package_src, \"metadata_generated.json\")):\n",
    "            print(\"Updating metadata_generated.json\")\n",
    "            metadata_generated = read_file(\n",
    "                os.path.join(package_src, \"metadata_generated.json\"), \"json\"\n",
    "            )\n",
    "\n",
    "            additional_info_path = os.path.join(package_dest, \"additional_info.json\")\n",
    "            if os.path.exists(additional_info_path):\n",
    "                additional_info = read_file(additional_info_path, \"json\")\n",
    "\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"] = {}\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\"name\"] = (\n",
    "                    \"metadata_generated\"\n",
    "                )\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\n",
    "                    \"description\"\n",
    "                ] = \"Descripción de los datos / Diccionario de datos\"\n",
    "                additional_info[\"metadata_resources\"][\"metadata_generated\"][\n",
    "                    \"format\"\n",
    "                ] = \"json\"\n",
    "\n",
    "                write_file(additional_info_path, additional_info, \"json\", \"utf-8\")\n",
    "                write_file(\n",
    "                    os.path.join(package_dest, \"metadata_generated.json\"),\n",
    "                    metadata_generated,\n",
    "                    \"json\",\n",
    "                    \"utf-8\",\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Celda de Prueba para generación de Concepto de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.helper import read_file\n",
    "def load_additional_info(directory):\n",
    "    \"\"\"Loads the additional_info.json file from the directory.\"\"\"\n",
    "    filepath = os.path.join(directory, \"additional_info.json\")\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"additional_info.json not found in {directory}\")\n",
    "    return read_file(filepath, \"json\")\n",
    "    \n",
    "datasets_directory = \"PipelineDatasets/SelectedDatasets\"\n",
    "enriched_datasets_directory = \"PipelineDatasets/EnrichedDatasets\"\n",
    "interest_word=\"transparencia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MetadataLLM.column_concept import ColumnConceptGenerator\n",
    "\n",
    "column_concepts_generator = ColumnConceptGenerator(DEVICE)\n",
    "\n",
    "# Few shots. TODO: Agregar más, y mejores.\n",
    "few_shots_column_concept = \"\"\"\n",
    "#### Ejemplo 1:\n",
    "Nombre de la Tabla: Proporción de jóvenes que tuvieron acceso a sustancias por tipo de sustancia.\n",
    "Contexto: Proporción de jóvenes que tuvieron acceso a sustancias por tipo de sustancia. Las sustancias consideradas son: Marihuana, cocaína, pastillas, pegamento, pasta base y otras\n",
    "Nombre Columna: Sustancia\n",
    "Valores:\n",
    "- Sustancia\n",
    "- Pegamento\n",
    "- Pastillas\n",
    "- Cocaína\n",
    "- Pasta base\n",
    "- Marihuana\n",
    "\n",
    "### Concepto sugerido:\n",
    "psychoactive drug\n",
    "\n",
    "#### Ejemplo 2:\n",
    "Nombre de la Tabla: Porcentaje de jóvenes que tienen cuenta de e-mail, Facebook y Twitter.\n",
    "Contexto: Proporción de jóvenes que tienen cuenta de e-mail, Facebook y Twitter según quintiles de ingreso.\n",
    "Nombre Columna: Redes Sociales\n",
    "Valores:\n",
    "- Twitter\n",
    "- e-mail\n",
    "- Facebook\n",
    "\n",
    "### Concepto sugerido:\n",
    "social media\n",
    "'''\n",
    "\n",
    "# Tomamos un directorio random de datasets_directory\n",
    "directory = os.path.join(datasets_directory, \"1f46180c-5e9a-41eb-a730-40fef51e63c0\")\n",
    "column_name = \"Descripcion\"\n",
    "\n",
    "additional_info = load_additional_info(directory)\n",
    "table_resources = additional_info.get(\"table_resources\", {})\n",
    "\n",
    "if len(table_resources) == 0:\n",
    "    print(f\"No resources found for package {package_id}\")\n",
    "    exit()\n",
    "\n",
    "# Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "table_id = list(table_resources.keys())[0]\n",
    "table = pd.read_csv(os.path.join(directory, f\"table_{table_id}.csv\"))\n",
    "\n",
    "# Metadata\n",
    "metadata_resources = additional_info.get(\"metadata_resources\", {})\n",
    "\n",
    "if len(metadata_resources) == 0:\n",
    "    print(f\"No metadata resources found for package {package_id}\")\n",
    "else:\n",
    "    metadata_id = list(metadata_resources.keys())[0]\n",
    "    metadata = read_file(\n",
    "        os.path.join(directory, f\"metadata_{metadata_id}.json\"), \"json\"\n",
    "    )\n",
    "\n",
    "column_concept = column_concepts_generator.generate_concept(\n",
    "    table, table_id, metadata, additional_info, column_name, few_shots_column_concept\n",
    ")\n",
    "\n",
    "print(column_concept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "# Cargar modelo de Sentence Transformers\n",
    "# embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embedding_model = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba con varias formas de escribir descripción\n",
    "def get_description(attr):\n",
    "    if attr.get(\"descripcion\", None) != None:\n",
    "        return attr[\"descripcion\"]\n",
    "    elif attr.get(\"Descripcion\", None) != None:\n",
    "        return attr[\"Descripcion\"]\n",
    "    elif attr.get(\"descripción\", None) != None:\n",
    "        return attr[\"descripción\"]\n",
    "    elif attr.get(\"Descripción\", None) != None:\n",
    "        return attr[\"Descripción\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def stringify(metadata, table, additional_info, metadata_info, metadata_texts):\n",
    "    stringified_metadata = \"\"\n",
    "    count = 0\n",
    "    for attr in metadata[\"atributos\"]:\n",
    "        atributo = attr[\"nombreDeAtributo\"]\n",
    "        descripcion = get_description(attr)\n",
    "        stringified_metadata += f\"{atributo}: {descripcion}\\n\"\n",
    "        valores_columna = table.iloc[:, count].unique()\n",
    "        stringified_metadata += (\n",
    "            f\"Valores de la columna: {', '.join(map(str, valores_columna))} \\n \\n\"\n",
    "        )\n",
    "        count += 1\n",
    "\n",
    "    metadata_texts.append(stringified_metadata)\n",
    "    metadata_info.append(additional_info)\n",
    "\n",
    "    return metadata_texts, metadata_info\n",
    "\n",
    "\n",
    "# Función para cargar metadatos desde subdirectorios y preparar textos para el embedding\n",
    "def load_and_prepare_data(base_directory):\n",
    "    metadata_texts = []\n",
    "    metadata_info = []\n",
    "\n",
    "    # Iterar a través de cada subdirectorio en el directorio base\n",
    "    for id_package in os.listdir(base_directory):\n",
    "        package_path = os.path.join(base_directory, id_package)\n",
    "        if os.path.isdir(package_path):  # Asegurarse de que es un directorio\n",
    "            for filename in os.listdir(package_path):\n",
    "                if filename.startswith(\"additional_info\"):\n",
    "                    filepath = os.path.join(package_path, filename)\n",
    "                    with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
    "                        data = json.load(file)\n",
    "                        data[\"id\"] = id_package\n",
    "                        title = data.get(\"title\", \"\")\n",
    "                        notes = data.get(\"notes\", \"\")\n",
    "                        organization = data.get(\"organization\", \"\")\n",
    "                        table_description = \" \".join(\n",
    "                            res[\"description\"]\n",
    "                            for res in data[\"table_resources\"].values()\n",
    "                        )\n",
    "\n",
    "                        full_text = f\"Titulo: {title} - Descripcion: {notes} - Organizacion: {organization} - Tabla: {table_description}\"\n",
    "                        metadata_texts.append(full_text)\n",
    "                        metadata_info.append(data)\n",
    "\n",
    "                        # Agarrar la key del primer metadata resources\n",
    "                        metadata_path = (\n",
    "                            f\"{list(data['metadata_resources'].keys())[0]}.json\"\n",
    "                        )\n",
    "                        if \"metadata\" in metadata_path:\n",
    "                            metadata = json.load(\n",
    "                                open(\n",
    "                                    os.path.join(package_path, metadata_path),\n",
    "                                    \"r\",\n",
    "                                    encoding=\"utf-8\",\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            metadata = json.load(\n",
    "                                open(\n",
    "                                    os.path.join(\n",
    "                                        package_path, f\"metadata_{metadata_path}\"\n",
    "                                    ),\n",
    "                                    \"r\",\n",
    "                                    encoding=\"utf-8\",\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "                        table = pd.read_csv(\n",
    "                            os.path.join(\n",
    "                                package_path,\n",
    "                                f\"table_{list(data['table_resources'].keys())[0]}.csv\",\n",
    "                            )\n",
    "                        )\n",
    "                        # Concatenar información relevante\n",
    "                        metadata_texts, metadata_info = stringify(\n",
    "                            metadata, table, data, metadata_info, metadata_texts\n",
    "                        )\n",
    "\n",
    "    return metadata_texts, metadata_info\n",
    "\n",
    "\n",
    "# Cargar los datos\n",
    "base_directory = f\"PipelineDatasets/FinalDatasets\"\n",
    "metadata_texts, metadata_info = load_and_prepare_data(base_directory)\n",
    "\n",
    "print(metadata_texts)\n",
    "print(metadata_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appendear \"passage\" al inicio de cada texto\n",
    "metadata_texts = [\"passage: \" + text for text in metadata_texts]\n",
    "\n",
    "metadata_embeddings = embedding_model.encode(metadata_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def find_closest_resource(query, k=2):\n",
    "    \"\"\"\n",
    "    Encuentra los recursos más similares a una consulta según embeddings.\n",
    "\n",
    "    Parámetros:\n",
    "        query (str): Texto de la consulta.\n",
    "        metadata_embeddings (np.array): Matriz de embeddings de los metadatos.\n",
    "        metadata_info (list): Lista de información asociada a los metadatos.\n",
    "        embedding_model: Modelo de embeddings usado para codificar el texto.\n",
    "        k (int): Número máximo de resultados a devolver.\n",
    "        threshold (float): Similitud mínima para considerar un resultado.\n",
    "\n",
    "    Retorna:\n",
    "        list: Lista de tuplas con (metadato, similitud), ordenadas por relevancia.\n",
    "    \"\"\"\n",
    "    # Obtener el embedding de la consulta\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "\n",
    "    # Calcular similitud coseno\n",
    "    similarities = cosine_similarity(query_embedding, metadata_embeddings)[0]\n",
    "\n",
    "    # Obtener los índices ordenados por similitud descendente\n",
    "    sorted_indices = np.argsort(similarities)[::-1]  # De mayor a menor\n",
    "\n",
    "    max_similarity = similarities[sorted_indices[0]]\n",
    "\n",
    "    results = []\n",
    "    for i in sorted_indices:\n",
    "        if max_similarity - similarities[i] < 0.002:\n",
    "            results.append((metadata_info[i], similarities[i]))\n",
    "            print(\"---------------------------\")\n",
    "            print(\"Text\")\n",
    "            print(metadata_texts[i])\n",
    "            print(\"Info\")\n",
    "            print(metadata_info[i])\n",
    "            print(\"Similarities\")\n",
    "            print(similarities[i])\n",
    "            print(\"---------------------------\")\n",
    "\n",
    "    # Retornar los k mejores resultados que cumplan el umbral\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "# Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#  load_in_4bit=True,\n",
    "#  bnb_4bit_quant_type=\"nf4\",\n",
    "#  bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# Inicializar el tokenizer\n",
    "tokenizer = ModelManager.tokenizer\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = ModelManager.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Busca con los indices generados por D3L\n",
    "def syntactic_query(resource):\n",
    "    # Tomar la primera key de table_resources (es la única porque elegimos solo una tabla)\n",
    "    table_id = list(resource[\"table_resources\"].keys())[0]\n",
    "    table_name = f\"table_{table_id}\"\n",
    "\n",
    "    # Searched results, K = 2\n",
    "    qe = QueryEngine(\n",
    "        name_index, value_index, embedding_index, format_index, distribution_index\n",
    "    )\n",
    "    results, extended_results = qe.table_query(\n",
    "        table=dataloader.read_table(table_name=table_name),\n",
    "        aggregator=None,\n",
    "        k=10,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Remove the same table from the results\n",
    "    results = [result for result in results if result[0] != table_name]\n",
    "\n",
    "    filtered_average_scores = []\n",
    "    # Filter the tables with an average of the 5 scores which is smaller than 0.5\n",
    "    for result in results:\n",
    "        scores = result[1]\n",
    "        average_score = np.mean(scores)\n",
    "        if average_score > 0.5:\n",
    "            filtered_average_scores.append((result[0], scores))\n",
    "\n",
    "    if filtered_average_scores != []:\n",
    "        print(\"Syntactic query found:\", results)\n",
    "\n",
    "    # Read the additional info from the resources that remain on filtered_average_scores\n",
    "    # Is necessary to do a greedy search on the folders of base_directory to look for the directory with the\n",
    "    # same name as the table, and get the additional info from there\n",
    "    # TODO: Do it more efficiently\n",
    "    for table_name, _ in filtered_average_scores:\n",
    "        for id_package in os.listdir(base_directory):\n",
    "            package_path = os.path.join(base_directory, id_package)\n",
    "            if os.path.isdir(package_path):\n",
    "                if f\"{table_name}.csv\" in os.listdir(package_path):\n",
    "                    data = load_additional_info(package_path)\n",
    "                    # Acceder al único recurso en 'table_resources'\n",
    "                    single_resource = next(iter(data[\"table_resources\"].values()))\n",
    "                    full_text = f\"\"\"\n",
    "        #### Recurso extra:\n",
    "            - Título: {data[\"title\"]}\n",
    "            - Organización: {data[\"organization\"]}\n",
    "            - Detalles: {data[\"notes\"]}\n",
    "            - URL del CSV con los datos: {single_resource[\"url\"]}\\n\\n\"\n",
    "        \"\"\"\n",
    "\n",
    "                    return full_text\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def prompt_tuning(query, closest_resources):\n",
    "    # Preparar la cabecera del prompt con la query del usuario\n",
    "    tuning = f'''\n",
    "    Eres un asistente especializado en la búsqueda de datos en el **Catálogo de Datos Abiertos de Uruguay**.\n",
    "\n",
    "    ### Instrucciones:\n",
    "      - Responde la consulta del usuario: **\"{query}\"** utilizando exclusivamente la información disponible en los recursos proporcionados.\n",
    "      - No inventes datos. Si no puedes responder con la información dada, menciona que no se encontró información específica.\n",
    "      - La respuesta debe ser clara, breve y útil para el usuario.\n",
    "      - Sugiere los recursos disponibles con su **URL** para que el usuario pueda explorarlos.\n",
    "      - **No** incluyas URLs que no estén dentro de la sección **\"Información disponible\"**.\n",
    "      - Asegurate de solo hacer uso de los links que se proveen en la sección **\"Información disponible\"**.\n",
    "      - **No** devuelvas explícitamente la tabla de datos en la respuesta.\n",
    "      - La respuesta no debe exceder **200 palabras**.\n",
    "\n",
    "    ### Información para usar en la respuesta:\n",
    "    '''\n",
    "    i = 1\n",
    "    for resource, _ in closest_resources:\n",
    "        # Acceder al único recurso en 'table_resources'\n",
    "        single_resource = next(iter(resource[\"table_resources\"].values()))\n",
    "        tuning += f\"\"\"\n",
    "        #### Recurso {i}:\n",
    "            - Título: {resource[\"title\"]}\n",
    "            - Organización: {resource[\"organization\"]}\n",
    "            - Detalles: {resource[\"notes\"]}\n",
    "            - URL del CSV con los datos: {single_resource[\"url\"]}\\n\\n\n",
    "            - Tabla:\n",
    "            {pd.read_csv(os.path.join(base_directory, f\"{resource['id']}\", f\"table_{list(resource['table_resources'].keys())[0]}.csv\"))}\n",
    "        \"\"\"\n",
    "        i += 1\n",
    "\n",
    "    tuning += f\"\"\"{syntactic_query(closest_resources[0][0])}\"\"\"\n",
    "\n",
    "    tuning += f\"\"\" Ahora genera la respuesta para la query del usuario, usando la informacion dada arriba: {query} \\n\"\"\"\n",
    "    tuning += \"### Respuesta \\n\"\n",
    "    return tuning\n",
    "\n",
    "\n",
    "# Función para generar texto con el modelo cuantizado\n",
    "def generate_text_with_model(prompt):\n",
    "    # Codificar el prompt en tokens\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Mover los tensores al dispositivo adecuado\n",
    "    inputs = {key: val.to(model.device) for key, val in inputs.items()}\n",
    "\n",
    "    # Configurar parámetros de generación\n",
    "    generation_parameters = {\n",
    "        \"max_new_tokens\": 350,\n",
    "        \"temperature\": 0.05,  # Mantiene el output más predecible\n",
    "        \"top_p\": 0.5,  # Reduce la probabilidad de que el modelo explore opciones raras\n",
    "        \"top_k\": 10,  # Limita las opciones a los 10 tokens más probables\n",
    "        \"repetition_penalty\": 1.1,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # Generar respuesta con el modelo\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, **generation_parameters)\n",
    "\n",
    "    # Decodificar los tokens generados en texto\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Consulta de ejemplo y uso del modelo para generar texto\n",
    "prefix = \"Sugiereme conjuntos de datos que contengan información de:\"\n",
    "query = \"acodike supergas\"\n",
    "closest_resources = find_closest_resource(query)\n",
    "\n",
    "if closest_resources:\n",
    "    print(f\"Los recursos más cercanos a la consulta '{query}' son:\")\n",
    "    for resource, distance in closest_resources:\n",
    "        print(f\"Título: {resource['title']}\")\n",
    "        print(f\"Organización: {resource['organization']}\")\n",
    "        print(f\"Detalles: {resource['notes']}\")\n",
    "    prompt = prompt_tuning(prefix + \" \" + query, closest_resources)\n",
    "    response_text = generate_text_with_model(prompt)\n",
    "    print(prompt)\n",
    "\n",
    "    response = response_text.split(\"### Respuesta\")[-1]\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"No se encontraron recursos relevantes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descargar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zip -r EnrichedDatasets.zip /content/PipelineDatasets/EnrichedDatasets\n",
    "# !zip -r SelectedDatasets.zip /content/PipelineDatasets/SelectedDatasets\n",
    "# !zip -r FinalDatasets.zip /content/PipelineDatasets/FinalDatasets\n",
    "\n",
    "# from google.colab import files\n",
    "# files.download('EnrichedDatasets.zip')\n",
    "# files.download('SelectedDatasets.zip')\n",
    "# files.download('FinalDatasets.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.dataset_evaluator import DatasetEvaluator\n",
    "import os\n",
    "\n",
    "# Crear dataset de evaluación\n",
    "dataset_evaluator = DatasetEvaluator(\"PipelineDatasets/SelectedDatasets\")\n",
    "\n",
    "dataset_evaluator.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BertScore con descripciones de los datasets\n",
    "import evaluate\n",
    "\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "\n",
    "def evaluate_generated_descriptions(generated, references):\n",
    "    scores = bertscore.compute(predictions=generated, references=references, lang=\"es\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "generated = [\n",
    "    'Esta tabla recopila datos relacionados con patrocinios públicos realizados durante varios años. La tabla incluye columnas para año, patrocinio público, valor en dólares e ingreso correspondientes en pesos, aunque estos últimos están marcados como sin valores asociados (\"N/C\"). El'\n",
    "]\n",
    "references = [\n",
    "    \"Información sobre inversiones en publicidad por año realizadas por ANCAP\"\n",
    "]\n",
    "\n",
    "evaluate_generated_descriptions(generated, references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar las \"notes\" del additional info de todos los archivos del groundTruth contra las de final_datasets_directory\n",
    "def evaluate_notes(ground_truth_directory, final_datasets_directory, package_ids):\n",
    "    evaluation = {}\n",
    "    for package in package_ids:\n",
    "        ground_truth_path = os.path.join(\n",
    "            ground_truth_directory, package, \"additional_info.json\"\n",
    "        )\n",
    "        enriched_path = os.path.join(\n",
    "            final_datasets_directory, package, \"additional_info.json\"\n",
    "        )\n",
    "\n",
    "        with open(ground_truth_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            ground_truth = json.load(file)\n",
    "\n",
    "        with open(enriched_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            enriched = json.load(file)\n",
    "\n",
    "        ground_truth_notes = ground_truth.get(\"notes\", \"\")\n",
    "        if ground_truth_notes == \"\":\n",
    "            print(\"Empty notes on \", package)\n",
    "            continue\n",
    "\n",
    "        enriched_notes = enriched.get(\"notes\", \"\")\n",
    "\n",
    "        scores = evaluate_generated_descriptions([enriched_notes], [ground_truth_notes])\n",
    "        evaluation[package] = scores\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "ground_truth_directory = \"PipelineDatasets/groundTruth\"\n",
    "package_ids = files_with_nothing + files_with_metadata\n",
    "notes_evaluation = evaluate_notes(\n",
    "    ground_truth_directory, os.path.join(final_datasets_directory), package_ids\n",
    ")\n",
    "\n",
    "# Imprimir rendimiento individual\n",
    "# print(notes_evaluation)\n",
    "\n",
    "# for package, scores in notes_evaluation.items():\n",
    "#     print(f\"Package: {package}\")\n",
    "#     print(f\"Precision: {scores['precision']}\")\n",
    "#     print(f\"Recall: {scores['recall']}\")\n",
    "#     print(f\"F1: {scores['f1']}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "# Average\n",
    "precision_sum = 0\n",
    "recall_sum = 0\n",
    "f1_sum = 0\n",
    "\n",
    "for scores in notes_evaluation.values():\n",
    "    precision_sum += scores[\"precision\"][0]\n",
    "    recall_sum += scores[\"recall\"][0]\n",
    "    f1_sum += scores[\"f1\"][0]\n",
    "\n",
    "print(f\"For a total of {len(notes_evaluation.values())} descriptions:\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Average precision: {precision_sum / len(notes_evaluation)}\")\n",
    "print(f\"Average recall: {recall_sum / len(notes_evaluation)}\")\n",
    "print(f\"Average f1: {f1_sum / len(notes_evaluation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetsUtils.helper import detect_encoding\n",
    "\n",
    "\n",
    "# Prueba con varias formas de escribir descripción\n",
    "def get_description(attr):\n",
    "    if attr.get(\"descripcion\", None) != None:\n",
    "        return attr[\"descripcion\"]\n",
    "    elif attr.get(\"Descripcion\", None) != None:\n",
    "        return attr[\"Descripcion\"]\n",
    "    elif attr.get(\"descripción\", None) != None:\n",
    "        return attr[\"descripción\"]\n",
    "    elif attr.get(\"Descripción\", None) != None:\n",
    "        return attr[\"Descripción\"]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Evalua las descripciones de cada columna de los datasets, comparando con las descripciones generadas en el ground truth y dadas.\n",
    "def evaluate_metadata(ground_truth_directory, final_datasets_directory, package_ids):\n",
    "    evaluation = {}\n",
    "    for package in package_ids:\n",
    "        ground_truth_path = os.path.join(ground_truth_directory, package)\n",
    "        loaded_additional_info = load_additional_info(ground_truth_path)\n",
    "\n",
    "        if len(loaded_additional_info.get(\"metadata_resources\", {})) == 0:\n",
    "            continue\n",
    "        ground_truth_metadata = list(\n",
    "            loaded_additional_info[\"metadata_resources\"].keys()\n",
    "        )[0]\n",
    "\n",
    "        enriched_metadata = os.path.join(\n",
    "            final_datasets_directory, package, \"metadata_generated.json\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            ground_truth = read_file(\n",
    "                os.path.join(\n",
    "                    ground_truth_path, f\"metadata_{ground_truth_metadata}.json\"\n",
    "                ),\n",
    "                \"json\",\n",
    "            )\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        enriched = read_file(enriched_metadata, \"json\")\n",
    "\n",
    "        ground_truth_attributes = ground_truth.get(\"atributos\", [])\n",
    "        enriched_attributes = enriched.get(\"atributos\", [])\n",
    "\n",
    "        ground_truth_descriptions = [\n",
    "            get_description(attr) for attr in ground_truth_attributes\n",
    "        ]\n",
    "        enriched_descriptions = [attr[\"descripcion\"] for attr in enriched_attributes]\n",
    "\n",
    "        # Truncate enriched_descriptions to the len of ground_truth\n",
    "        enriched_descriptions = enriched_descriptions[: len(ground_truth_descriptions)]\n",
    "\n",
    "        # Borrar todos los \"\" de ground_truth_descriptions, y borrar el mismo indice en enriched_descriptions\n",
    "        for i in range(len(ground_truth_descriptions) - 1, -1, -1):\n",
    "            if ground_truth_descriptions[i] == \"\":\n",
    "                ground_truth_descriptions.pop(i)\n",
    "                enriched_descriptions.pop(i)\n",
    "\n",
    "        print(\"Visualizar descripciones\")\n",
    "        for i in range(len(enriched_descriptions) - 1, -1, -1):\n",
    "            # Imprimir ambas descripciones para visualizarlo\n",
    "            print(\"\\n\")\n",
    "            print(f\"Ground truth: {ground_truth_descriptions[i]}\")\n",
    "            print(f\"Enriched: {enriched_descriptions[i]}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        scores = evaluate_generated_descriptions(\n",
    "            enriched_descriptions, ground_truth_descriptions\n",
    "        )\n",
    "        evaluation[package] = scores\n",
    "\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "package_ids = files_with_nothing + files_with_notes\n",
    "metadata_evaluation = evaluate_metadata(\n",
    "    ground_truth_directory, os.path.join(final_datasets_directory), package_ids\n",
    ")\n",
    "print(metadata_evaluation)\n",
    "\n",
    "precision_global = []\n",
    "recall_global = []\n",
    "f1_global = []\n",
    "descriptions_count = 0\n",
    "\n",
    "for package, scores in metadata_evaluation.items():\n",
    "    print(f\"Package: {package}\")\n",
    "    precision_average = sum(scores[\"precision\"]) / len(scores[\"precision\"])\n",
    "    print(f\"Precision average: {precision_average}\")\n",
    "    recall_average = sum(scores[\"recall\"]) / len(scores[\"recall\"])\n",
    "    print(f\"Recall average: {recall_average}\")\n",
    "    f1_average = sum(scores[\"f1\"]) / len(scores[\"f1\"])\n",
    "    print(f\"F1 average: {f1_average}\")\n",
    "    print(\"\\n\")\n",
    "    precision_global.append(precision_average)\n",
    "    recall_global.append(recall_average)\n",
    "    f1_global.append(f1_average)\n",
    "    descriptions_count += len(scores[\"precision\"])\n",
    "\n",
    "print(f\"For a total of {descriptions_count} descriptions:\")\n",
    "print(\"\\n\")\n",
    "print(f\"Average global precision: {sum(precision_global) / len(precision_global)}\")\n",
    "print(f\"Average global recall: {sum(recall_global) / len(recall_global)}\")\n",
    "print(f\"Average global f1: {sum(f1_global) / len(f1_global)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
